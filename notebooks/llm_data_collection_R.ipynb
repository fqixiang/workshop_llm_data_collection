{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fqixiang/workshop_llm_data_collection/blob/main/notebooks/llm_data_collection_R.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Large Language Models for Data Collection in Social Sciences"
      ],
      "metadata": {
        "id": "PHhOlhjzjH0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and load required packages\n",
        "install.packages(c(\"ellmer\", \"patchwork\", \"irr\"))\n",
        "library(ellmer) # for LLM API interfacing in R\n",
        "library(patchwork) # for combining multiple ggplot visualizations,\n",
        "library(irr) # for computing inter-rater reliability metrics such as Cohen’s Kappa or Krippendorff’s Alpha.\n",
        "\n",
        "# Loads the 'tidyverse' collection of packages for data manipulation and visualization.\n",
        "library(tidyverse)"
      ],
      "metadata": {
        "id": "BzSixTsZHN1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gabrielle Martins van Jaarsveld's SoDa fellowship dataset\n",
        "\n",
        "Feel free to use your own data. By default, we will use a toy dataset from Gabrielle Martins van Jaarsveld's SoDa fellowship project on annotating markers of self-regulated learning from student conversation data.\n",
        "\n",
        "This dataset contains the following columns:\n",
        "\n",
        "- `id`: The id of the row/conversation/student.\n",
        "\n",
        "- `conversation`: The text of the conversations based on which specificity scores are derived (by humans or LLMs).\n",
        "\n",
        "- `score_specificity_llm`: The specificity score of a conversation based on carefully prompted response from LLMs. It varies between 0, 1 and 2.\n",
        "\n",
        "- `score_specificity_human`: The specificity score of a conversation based on human expert annotators. It is treated as gold standard (i.e., free from measurement error). It varies between 0, 1 and 2.\n",
        "\n",
        "- `performance`: The academic performance of a student, varying from 1 to 10."
      ],
      "metadata": {
        "id": "GE3OlfYOlb0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data into a dataframe."
      ],
      "metadata": {
        "id": "eK6gCQI9gmAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download our example data\n",
        "data_url <- \"https://sodascience.github.io/workshop_llm_data_collection/data/srl_data_example.csv\"\n",
        "\n",
        "# Read CSV into dataframe\n",
        "df <- read_csv(data_url)"
      ],
      "metadata": {
        "id": "w6iO_UFkm2aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that only the first 10 rows contain the text of the conversations. We will use these texts for the prompting experiments to come.\n",
        "\n",
        "Display the first 10 rows of the dataset."
      ],
      "metadata": {
        "id": "z0lSfXdFEb7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head(df, 10)"
      ],
      "metadata": {
        "id": "kvZ5ZgGIEepO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using ellmer to call OpenAI's API\n",
        "\n",
        "We will be using the R package `ellmer` to perform our prompting experiments. One great advantage of using `ellmer` is that it takes away the trouble of having to learn different LLM APIs. Instead, it allows you to call different LLM APIs (both commercial and open-source) effortlessly (relatively speaking) with very simple modifications of your `ellmer` code!\n",
        "\n",
        "We will be calling OpenAI's LLM in this notebook. Feel free to experiment with other APIs and models! To do so, check out https://ellmer.tidyverse.org/."
      ],
      "metadata": {
        "id": "KhiH0VRqljn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first configure your OpenAI API key. Enter when being prompted. Don't have one? Ask the workshop instructors!"
      ],
      "metadata": {
        "id": "VUBe3-xbMq-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt user for API key\n",
        "openai_api_key <- readline(prompt = \"Enter API key for OpenAI: \")\n",
        "Sys.setenv(OPENAI_API_KEY = openai_api_key)"
      ],
      "metadata": {
        "id": "HPhAIQkW_MNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now define a function that makes a call to the OpenAI API for gpt-4o-mini."
      ],
      "metadata": {
        "id": "5zqgVUqjBwXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "call_openai_api <- function(system_prompt, user_prompt) {\n",
        "  chat <- chat_openai(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    system_prompt = system_prompt,\n",
        "    api_args = list(temperature = 0,\n",
        "                    max_tokens = 1000,\n",
        "                    seed = 42),\n",
        "    echo = \"none\" #suppress the output from being printed\n",
        "  )\n",
        "  return(chat$chat(user_prompt))\n",
        "}"
      ],
      "metadata": {
        "id": "7_NDChS3_-yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with a single prompt"
      ],
      "metadata": {
        "id": "r5bCH0msmBNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with the system prompt (i.e., high-level instruction to the model)."
      ],
      "metadata": {
        "id": "W7pQ9bJjeYpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt <- \"\n",
        "You are an expert in educational assessment and goal evaluation, with\n",
        "specialized expertise in applying deductive coding schemes to score the quality\n",
        "and content of student goals.\n",
        "\n",
        "##TASK##\n",
        "A university student was given a series of prompts, guiding them through the\n",
        "process of setting and elaborating on an academic goal for the coming week. You\n",
        "will be provided with the entire conversation including the prompts, and the\n",
        "student answers. Your objective is to assess the specificity of of the student’s\n",
        "goal on a scale of 0 to 2 based on the entire conversation.\n",
        "\""
      ],
      "metadata": {
        "id": "6DHz1SuRp964"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a prompt request with the system prompt and the user prompt based on the first conversation from the dataset."
      ],
      "metadata": {
        "id": "VWHolr5NetK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response <- call_openai_api(\n",
        "  system_prompt = system_prompt,\n",
        "  user_prompt   = df[[\"conversation\"]][1]\n",
        ")\n",
        "cat(response)"
      ],
      "metadata": {
        "id": "Chb3505UWnfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voila! You have your first successful prompting interaction with the API of a large language model!"
      ],
      "metadata": {
        "id": "1MfKrGf3e4MZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with multiple prompts\n",
        "Next, we are going beyond a single prompt. Instead, we will work with **multiple prompts** at the same time!"
      ],
      "metadata": {
        "id": "teHsF27YceX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run a for loop through all the 10 conversations."
      ],
      "metadata": {
        "id": "riOZIhzhKlZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize output list\n",
        "responses <- list()\n",
        "\n",
        "for (i in 1:10) {\n",
        "  # extract current conversation and id\n",
        "  current_convo <- df |> slice(i) |> pull(\"conversation\")\n",
        "  current_id    <- df |> slice(i) |> pull(\"id\")\n",
        "\n",
        "  # get response from llm\n",
        "  response <- call_openai_api(\n",
        "    system_prompt = system_prompt,\n",
        "    user_prompt   = current_convo\n",
        "  )\n",
        "\n",
        "  # assign to output list\n",
        "  responses[[current_id]] <- response\n",
        "\n",
        "  # report progress (does not work well in colab)\n",
        "  message(sprintf(\"%d/10 completed\", i))\n",
        "}"
      ],
      "metadata": {
        "id": "vhEsY-IwGUZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect the responses!"
      ],
      "metadata": {
        "id": "_9lYcc12LFht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat(responses[[2]])"
      ],
      "metadata": {
        "id": "GIYFus4DNJ-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using structured output with a single prompt"
      ],
      "metadata": {
        "id": "59gpSzXadJej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To force the LLM to produce outputs in formats specified by you, you need to use the `$extract_data()` method instead of the `$chat` method.\n",
        "\n",
        "Below, we define our desired output format as:\n",
        "- \"reasoning\": a string that provides the model's reasoning.\n",
        "- \"specificity_score\": an integer (either 0, 1 or 2) reflecting the specificity of a conversation."
      ],
      "metadata": {
        "id": "uuEez0lGTf78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_structure <- type_object(\n",
        "  specificity_score = type_integer(\"The specificity score of the entire conversation on a scale of 0, 1 and 2.\"),\n",
        "  reasoning = type_string(\"Your reasoning process.\")\n",
        ")\n",
        "\n",
        "call_openai_api_structured <- function(system_prompt, user_prompt) {\n",
        "  chat <- chat_openai(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    system_prompt = system_prompt,\n",
        "    api_args = list(temperature = 0,\n",
        "                    max_tokens = 1000,\n",
        "                    seed = 42),\n",
        "    echo = \"none\" #suppress the output from being printed\n",
        "  )\n",
        "  response <- chat$chat_structured(user_prompt, type = output_structure)\n",
        "  return(response)\n",
        "}"
      ],
      "metadata": {
        "id": "hbluX4wfdPqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try with a single prompt request."
      ],
      "metadata": {
        "id": "DHATrS3zUpML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structured_response = call_openai_api_structured(system_prompt, df[[\"conversation\"]][1])\n",
        "print(structured_response)"
      ],
      "metadata": {
        "id": "8qFPaYJCmRUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using structured output with muitiple prompts\n",
        "\n",
        "Being able to work with multiple prompts at the same time and obtain structured output will save you a substantial amount of time in research projects!"
      ],
      "metadata": {
        "id": "SWmyBbl0LnOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structured_responses <- list()\n",
        "for (i in 1:10) {\n",
        "  # extract current conversation and id\n",
        "  current_convo <- df |> slice(i) |> pull(\"conversation\")\n",
        "  current_id    <- df |> slice(i) |> pull(\"id\")\n",
        "\n",
        "  # get response from llm\n",
        "  response <- call_openai_api_structured(\n",
        "    system_prompt = system_prompt,\n",
        "    user_prompt   = current_convo\n",
        "  )\n",
        "\n",
        "  # assign to output list\n",
        "  structured_responses[[current_id]] <- response\n",
        "\n",
        "  # report progress (does not work well in colab)\n",
        "  message(sprintf(\"%d/10 completed\", i))\n",
        "}"
      ],
      "metadata": {
        "id": "X6zuVgkJrqVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turn the structured responses into a data frame and show it."
      ],
      "metadata": {
        "id": "mMyf6gljIsYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use map_dfr from purrr package to turn list into dataframe\n",
        "response_df <- map_dfr(structured_responses, I, .id = \"id\")\n",
        "response_df"
      ],
      "metadata": {
        "id": "5_NYNFRGsO2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check annotation quality"
      ],
      "metadata": {
        "id": "Z3eLGxm5dW1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `kripp.alpha()` function from the `irr` package can be used to calculate agreement (i.e., Krippendorff's Alpha) of specificity scores between two raters (e.g., LLMs and human experts)."
      ],
      "metadata": {
        "id": "iEHdFYVaI0cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the agreement between the specificity scores we got from the LLM above and the human expert-coded specificity scores!"
      ],
      "metadata": {
        "id": "JRNqIXIqJN6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create rating matrix (rows = raters, cols = items)\n",
        "rating_matrix <- rbind(\n",
        "  df |> slice(1:10) |> pull(score_specificity_human),\n",
        "  response_df |> pull(specificity_score)\n",
        ")\n",
        "\n",
        "# compute agreement (0 - 1)\n",
        "kripp.alpha(rating_matrix, method = \"interval\")"
      ],
      "metadata": {
        "id": "apooHATcs1wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not a great agreement score!\n",
        "\n",
        "How about the agreement between the LLM specificity scores that already came with the dataset (i.e., column `score_specificity_llm`) and the human expert-coded scores?\n",
        "\n",
        "Note that `score_specificity_llm` is based on prompts that were carefully engineered by Gabrielle."
      ],
      "metadata": {
        "id": "c4kQllO3JhML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rating_matrix <- rbind(\n",
        "  df |> slice(1:10) |> pull(score_specificity_human),\n",
        "  df |> slice(1:10) |> pull(score_specificity_llm)\n",
        ")\n",
        "\n",
        "kripp.alpha(rating_matrix, method = \"interval\")"
      ],
      "metadata": {
        "id": "Z97P-AtdnR4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow! Much better after some careful prompt engineering!"
      ],
      "metadata": {
        "id": "YMRpL4zIKZOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Try different prompting techniques to get better results!\n",
        "\n",
        "For example:\n",
        "\n",
        "1. Improve clarity & specificity\n",
        "2. Role-based prompting\n",
        "3. Step-by-step reasoning (Chain-of-Thought Prompting)\n",
        "4. Few-shot prompting\n",
        "5. Output structuring\n",
        "6. Self-consistency prompting\n",
        "\n",
        "Use the `kripp.alpha` function to check the LLM's annotation quality."
      ],
      "metadata": {
        "id": "8I28nCUtjkFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's write some code!"
      ],
      "metadata": {
        "id": "hTrRxc-LuktS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}