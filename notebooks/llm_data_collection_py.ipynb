{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHhOlhjzjH0H"
      },
      "source": [
        "# Using Large Language Models for Data Collection in Social Sciences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2e1RcGZrj4e5"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -qU langchain[openai] tqdm pydantic krippendorff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1B9Ok90OkqZC"
      },
      "outputs": [],
      "source": [
        "import getpass  # Provides a secure way to handle user passwords or other sensitive input without echoing them on the screen.\n",
        "import os  # Gives access to operating system functionalities like file paths, environment variables, and directory operations.\n",
        "import pandas as pd  # Imports the pandas library (aliased as pd) for handling and analyzing tabular data in DataFrames.\n",
        "import numpy as np  # Imports NumPy (aliased as np), a library for fast numerical computations and array manipulations.\n",
        "from tqdm import tqdm  # Imports tqdm, a progress bar utility that provides visual feedback for loops and long-running processes.\n",
        "from langchain.chat_models import init_chat_model  # Imports a function from LangChain to initialize a chat-based large language model (LLM) interface.\n",
        "from langchain_core.prompts import ChatPromptTemplate  # Imports a template class for creating structured prompts used in LLM interactions.\n",
        "from pydantic import BaseModel, Field  # Imports BaseModel (for defining structured data models) and Field (for specifying metadata and validation rules).\n",
        "import krippendorff  # Imports the krippendorff library, used to compute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE3OlfYOlb0G"
      },
      "source": [
        "## Gabrielle Martins van Jaarsveld's SoDa fellowship dataset\n",
        "\n",
        "Feel free to use your own data. By default, we will use a toy dataset from Gabrielle Martins van Jaarsveld's SoDa fellowship project on annotating markers of self-regulated learning from student conversation data.\n",
        "\n",
        "This dataset contains the following columns:\n",
        "\n",
        "- `id`: The id of the row/conversation/student.\n",
        "\n",
        "- `conversation`: The text of the conversations based on which specificity scores are derived (by humans or LLMs).\n",
        "\n",
        "- `score_specificity_llm`: The specificity score of a conversation based on carefully prompted response from LLMs. It varies between 0, 1 and 2.\n",
        "\n",
        "- `score_specificity_human`: The specificity score of a conversation based on human expert annotators. It is treated as gold standard (i.e., free from measurement error). It varies between 0, 1 and 2.\n",
        "\n",
        "- `performance`: The academic performance of a student, varying from 1 to 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-LyJLcM4RzT"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6iO_UFkm2aQ"
      },
      "outputs": [],
      "source": [
        "# url where you can download our example data\n",
        "data_url = \"https://sodascience.github.io/workshop_llm_data_collection/data/srl_data_example.csv\"\n",
        "\n",
        "# Read CSV into dataframe\n",
        "df = pd.read_csv(data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0lSfXdFEb7z"
      },
      "source": [
        "Display the first 10 rows of the dataset.\n",
        "\n",
        "Note that only the first 10 rows contain the text of the conversations. We will use these texts for the prompting experiments to come."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvZ5ZgGIEepO"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhiH0VRqljn2"
      },
      "source": [
        "## Using langchain to call OpenAI's API\n",
        "\n",
        "We will be using the Python package `langchain` to perform our prompting experiments. One great advantage of using `langchain` is that it takes away the trouble of having to learn different LLM APIs. Instead, it allows you to call different LLM APIs (both commercial and open-source) effortlessly (relatively speaking) with very simple modifications of your `langchain` code!\n",
        "\n",
        "We will be calling OpenAI's LLM in this notebook. Feel free to experiment with other APIs and models! To do so, check out https://python.langchain.com/docs/tutorials/chatbot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUBe3-xbMq-e"
      },
      "source": [
        "Initialise an LLM model. You need to enter your OpenAI API key for this when being prompted. Don't have one? Ask the workshop instructors!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D01Wli7ilI83"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
        "model = init_chat_model(\"gpt-4o-mini\",\n",
        "                        model_provider=\"openai\",\n",
        "                        temperature=0,\n",
        "                        max_tokens=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5bCH0msmBNC"
      },
      "source": [
        "## Working with a single prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7pQ9bJjeYpL"
      },
      "source": [
        "Let's start with the system prompt (i.e., high-level instruction to the model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DHz1SuRp964"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are an expert in educational assessment and goal evaluation, with\n",
        "specialized expertise in applying deductive coding schemes to score the quality\n",
        "and content of student goals.\n",
        "\n",
        "##TASK##\n",
        "A university student was given a series of prompts, guiding them through the\n",
        "process of setting and elaborating on an academic goal for the coming week. You\n",
        "will be provided with the entire conversation including the prompts, and the\n",
        "student answers. Your objective is to assess the specificity of of the studentâ€™s\n",
        "goal on a scale of 0 to 2 based on the entire conversation.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HivI2QseewJ"
      },
      "source": [
        "Use the prompt template module from langchain to create a prompt request with both **system** and **user** prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEQtu3AcWlVn"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"user\", \"{conversation}\")\n",
        "])\n",
        "\n",
        "# Create a prompt request with the system prompt and the user prompt based on\n",
        "# the first conversation from the dataset\n",
        "prompt_request = prompt_template.invoke({\"conversation\": df.iloc[0,1]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_CixxrPel8s"
      },
      "source": [
        "Check the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jam82IDfLz2Y"
      },
      "outputs": [],
      "source": [
        "prompt_request.to_messages()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWHolr5NetK-"
      },
      "source": [
        "Prompt the model and inspect the response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chb3505UWnfk"
      },
      "outputs": [],
      "source": [
        "response = model.invoke(prompt_request)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MfKrGf3e4MZ"
      },
      "source": [
        "Voila! You have your first successful prompting interaction with the API of a large language model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teHsF27YceX_"
      },
      "source": [
        "## Working with multiple prompts\n",
        "Next, we are going beyond a single prompt. Instead, we will work with **multiple prompts** at the same time!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hhigdP9THoO"
      },
      "source": [
        "Define a list of request IDs and another list of conversations (necessary for forming the user prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAfnxKlQf4eQ"
      },
      "outputs": [],
      "source": [
        "ids = df.id[:10].tolist()\n",
        "conversations = df.conversation[:10].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0b78F6Chy_s"
      },
      "outputs": [],
      "source": [
        "responses = {}\n",
        "for id, conversation in tqdm(zip(ids, conversations),\n",
        "                             total=len(ids),\n",
        "                             desc=\"Processing Requests\"):\n",
        "    prompt_request = prompt_template.invoke({\"conversation\": conversation})\n",
        "    response = model.invoke(prompt_request)\n",
        "    responses[id] = response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9lYcc12LFht"
      },
      "source": [
        "Inspect the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3KOt37Hi-UX"
      },
      "outputs": [],
      "source": [
        "print(responses['request_3'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59gpSzXadJej"
      },
      "source": [
        "## Using structured output with a single prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuEez0lGTf78"
      },
      "source": [
        "To force the LLM to produce outputs in formats specified by you, you need to use the `BaseModel` and `Field` classes from the `pydantic` package.\n",
        "\n",
        "Below, we define our desired output format as:\n",
        "- \"specificity_score\": an integer (either 0, 1 or 2) reflecting the specificity of a conversation.\n",
        "- \"reasoning\": a string that provides the model's reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbluX4wfdPqx"
      },
      "outputs": [],
      "source": [
        "class SpecificityFormat(BaseModel):\n",
        "    \"\"\"Always use this tool to structure your response to the user.\"\"\"\n",
        "    specificity_score: int = Field(description=\"The specificity score of the entire conversation on a scale of 0, 1 and 2.\")\n",
        "    reasoning: str = Field(description=\"Your reasoning process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHATrS3zUpML"
      },
      "source": [
        "Try with a single prompt request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qFPaYJCmRUs"
      },
      "outputs": [],
      "source": [
        "# Bind responseformatter schema to the model\n",
        "model_structured = model.with_structured_output(SpecificityFormat)\n",
        "\n",
        "# try to run a request throught this new model\n",
        "prompt_request = prompt_template.invoke({\"conversation\": df.iloc[0,1]})\n",
        "structured_response = model_structured.invoke(prompt_request)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1wxfPM6k7PJ"
      },
      "outputs": [],
      "source": [
        "dict(structured_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWmyBbl0LnOR"
      },
      "source": [
        "## Using structured output with multiple prompts\n",
        "\n",
        "Being able to work with multiple prompts at the same time and obtain structured output will save you a substantial amount of time in research projects!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6zuVgkJrqVV"
      },
      "outputs": [],
      "source": [
        "structured_responses = {}\n",
        "for id, conversation in tqdm(zip(ids, conversations), total=len(ids), desc=\"Processing Messages\"):\n",
        "    prompt_request = prompt_template.invoke({\"conversation\": conversation})\n",
        "    structured_response = model_structured.invoke(prompt_request)\n",
        "    # Below we save only the specificity scores\n",
        "    structured_responses[id] = dict(structured_response)[\"specificity_score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMyf6gljIsYV"
      },
      "source": [
        "Display all the structured responses (only the specificity scores)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_NYNFRGsO2d"
      },
      "outputs": [],
      "source": [
        "structured_responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3eLGxm5dW1f"
      },
      "source": [
        "## Check annotation quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hE7knNFJD54"
      },
      "source": [
        "Implement a handy function to calculate Krippendorff's Alpha (i.e., agreement) between two lists of specificity scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9zndWAmJBcQ"
      },
      "outputs": [],
      "source": [
        "def compute_krippendorff_alpha(x: list[int], y: list[int]):\n",
        "  # Format data into a reliability matrix (rows=raters, cols=items)\n",
        "  data_krippendorff = np.array([x, y])\n",
        "  # Compute Krippendorffâ€™s Alpha (interval metric)\n",
        "  kripp_alpha = krippendorff.alpha(reliability_data=data_krippendorff, level_of_measurement='interval')\n",
        "  return kripp_alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRNqIXIqJN6-"
      },
      "source": [
        "Let's check the agreement between the specificity scores we got from the LLM above and the human expert-coded specificity scores!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apooHATcs1wp"
      },
      "outputs": [],
      "source": [
        "score_specificity_human = df.score_specificity_human[:10].tolist()\n",
        "structured_response_values = list(structured_responses.values())\n",
        "print(\"Krippendorff's Alpha:\", compute_krippendorff_alpha(structured_response_values, score_specificity_human))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4kQllO3JhML"
      },
      "source": [
        "Not a great agreement score!\n",
        "\n",
        "How about the agreement between the LLM specificity scores that already came with the dataset (i.e., column `score_specificity_llm`) and the human expert-coded scores?\n",
        "\n",
        "Note that `score_specificity_llm` is based on prompts that were carefully engineered by Gabrielle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z97P-AtdnR4K"
      },
      "outputs": [],
      "source": [
        "score_specificity_llm = df.score_specificity_llm[:10].tolist()\n",
        "print(\"Krippendorff's Alpha:\", compute_krippendorff_alpha(score_specificity_llm, score_specificity_human))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMRpL4zIKZOr"
      },
      "source": [
        "Wow! Much better result!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I28nCUtjkFt"
      },
      "source": [
        "## Exercise: Try different prompting techniques to get better results!\n",
        "\n",
        "For example:\n",
        "\n",
        "1. Improve clarity & specificity\n",
        "2. Role-based prompting\n",
        "3. Step-by-step reasoning (Chain-of-Thought Prompting)\n",
        "4. Few-shot prompting\n",
        "5. Output structuring\n",
        "6. Self-consistency prompting\n",
        "\n",
        "Use the previous `compute_krippendorff_alpha` function to check the LLM's annotation quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTrRxc-LuktS"
      },
      "outputs": [],
      "source": [
        "# Let's write some code!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}